{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BoW - SVM",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE5CtJyOd-rw",
        "colab_type": "text"
      },
      "source": [
        "# Bag of Words with SVM Model\n",
        "This notebook documents the implementation of the Bag-of-Words Model and Support Vector Machine for the English-German task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYmoyswzen-N",
        "colab_type": "text"
      },
      "source": [
        "## 1. Importing libraries\n",
        "In this section, we import the nltk, numpy, re and heapq to use in our implementation later on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTPrIahceABy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk \n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import numpy as np\n",
        "import re\n",
        "import heapq\n",
        "stop_words_en = set(stopwords.words('english'))\n",
        "stop_words_de = set(stopwords.words('german'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T59pvgF3fGNX",
        "colab_type": "text"
      },
      "source": [
        "## 2. Defining functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTYIJZpjfMt9",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Read File\n",
        "In this function, we write a function that take in a file as input and output the text in the file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWecNZplfcWt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_file(file):\n",
        "    f = open(file,encoding=\"utf8\")\n",
        "    text = f.readlines()\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVoIqc9bhhi0",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Pre-Processing\n",
        "In this section, we do 3 types of pre-processing on the text:\n",
        "\n",
        "\n",
        "1.   Removal of stop words\n",
        "2.   Removal of punctuation\n",
        "3.   Conversion to lowercase\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G6RdCtmf3SX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_text(file,lang):\n",
        "    text = read_file(file)\n",
        "    for i in range(len(text)):\n",
        "      # Conversion to lowercase\n",
        "      text[i] = text[i].lower()\n",
        "      # Removal of punctuation\n",
        "      text[i] = re.sub(r'\\W',' ',text[i])\n",
        "      text[i] = re.sub(r'\\s+',' ',text[i])\n",
        "      tokens = nltk.word_tokenize(text[i])\n",
        "      # Removal of stop words\n",
        "      if lang == 'en':\n",
        "          text[i] = [w for w in tokens if not w in stop_words_en]\n",
        "      elif lang == 'de':\n",
        "          text[i] = [w for w in tokens if not w in stop_words_de]\n",
        "    cleaned_text = text\n",
        "    return cleaned_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3L-MJxbf5KF",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 Build Vocabulary of Known Words\n",
        "For this function, we build the vocabulary of known words from the corpus needed for the Bag-of-Words model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0t_kMXwgQ9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_vocab(file, lang):\n",
        "    cleaned_text = process_text(file,lang)\n",
        "    wordfreq = {}\n",
        "    for sentence in cleaned_text:\n",
        "        for token in sentence:\n",
        "            if token not in wordfreq.keys():\n",
        "                wordfreq[token] = 1\n",
        "            else:\n",
        "                wordfreq[token] += 1\n",
        "    vocab_list = heapq.nlargest(400, wordfreq, key=wordfreq.get)\n",
        "    return vocab_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTGSXsxDgUn_",
        "colab_type": "text"
      },
      "source": [
        "### 2.4 Vectorise Sentence\n",
        "In this function, we convert the sentences into vector form using the vocabulary of known words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3X5nrZDgifv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_to_vec(file,wordfreqlist,lang):\n",
        "    cleaned_text = process_text(file, lang)\n",
        "    sentence_vectors = []\n",
        "    for sentence in cleaned_text:\n",
        "        sent_vec = []\n",
        "        for token in wordfreqlist:\n",
        "            if token in sentence:\n",
        "                sent_vec.append(1)\n",
        "            else:\n",
        "                sent_vec.append(0)\n",
        "        sentence_vectors.append(sent_vec)\n",
        "    sentence_vectors = np.asarray(sentence_vectors)\n",
        "    return sentence_vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDBbDQCHisbd",
        "colab_type": "text"
      },
      "source": [
        "## 3. Getting Bag-of-Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EaG7ZjUiBY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get stopwords from nltk package\n",
        "stop_words_en = set(stopwords.words('english'))\n",
        "stop_words_de = set(stopwords.words('german'))\n",
        "\n",
        "# Get bag of words for English\n",
        "bow_en = build_vocab(\"./train.ende.src\", \"en\")\n",
        "\n",
        "# Get bag of words for German\n",
        "bow_de = build_vocab(\"./train.ende.mt\" , \"de\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRmKEKCAjWlF",
        "colab_type": "text"
      },
      "source": [
        "## 4. Getting Training and Validation Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQDvb-5jjc3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training Set - X \n",
        "de_train_src = convert_to_vec(\"./train.ende.src\",bow_en, \"en\")\n",
        "de_train_mt = convert_to_vec(\"./train.ende.mt\",bow_de,  \"de\")\n",
        "\n",
        "X_train_de = np.concatenate((np.array(de_train_src), np.array(de_train_mt)), 1)\n",
        "\n",
        "# Validation Set - X\n",
        "de_val_src = convert_to_vec(\"./dev.ende.src\",bow_en, \"en\")\n",
        "de_val_mt = convert_to_vec(\"./dev.ende.mt\",bow_de,  \"de\")\n",
        "\n",
        "X_val_de = np.concatenate((np.array(de_val_src), np.array(de_val_mt)), 1)\n",
        "\n",
        "# Combine Training and Validation Set - X\n",
        "X = np.concatenate((X_train_de, X_val_de), axis=0)\n",
        "\n",
        "# Training Set - y labels\n",
        "de_train_scores = read_file(\"./train.ende.scores\")\n",
        "train_scores = np.array(de_train_scores).astype(float)\n",
        "y_train_de = train_scores\n",
        "\n",
        "# Validation Set - y labels\n",
        "de_val_scores = read_file(\"./dev.ende.scores\")\n",
        "val_scores = np.array(de_val_scores).astype(float)\n",
        "y_val_de = val_scores\n",
        "\n",
        "# Combine Training and Validation Set - y labels\n",
        "y = np.concatenate((y_train_de, y_val_de), axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzLLKen-j10n",
        "colab_type": "text"
      },
      "source": [
        "## 5. Training the Regressor\n",
        "First, define the RMSE function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKeIwBm2j9eW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rmse(predictions, targets):\n",
        "    return np.sqrt(((predictions - targets) ** 2).mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cR9xD-jBj-Gb",
        "colab_type": "text"
      },
      "source": [
        "### 5.1 SVM\n",
        "Cross-validation is used to split the training and validation set into 8 folds and the model is then trained on these 8 folds. The one with the highest average Pearson correlation value corresponds to the best type of SVM. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJL6vmkikECs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "from scipy.stats.stats import pearsonr\n",
        "from sklearn.model_selection import KFold\n",
        "import pickle\n",
        "\n",
        "model = [0,0,0,0]\n",
        "for i, k in enumerate(['linear','poly','rbf','sigmoid']):\n",
        "  # split into 8 folds \n",
        "  kf = KFold(n_splits=8, shuffle=True, random_state=42)\n",
        "  pearson_arr = []\n",
        "  print(k)\n",
        "  LOCAL_MAX = -1\n",
        "  for train_index, val_index in kf.split(X):\n",
        "      X_train_de, X_val_de = X[train_index], X[val_index]\n",
        "      y_train_de, y_val_de = y[train_index], y[val_index]\n",
        "      clf_t = SVR(kernel=k,gamma='auto')\n",
        "      clf_t.fit(X_train_de, y_train_de)\n",
        "      predictions = clf_t.predict(X_val_de)\n",
        "      pearson = pearsonr(y_val_de, predictions)\n",
        "      # check if pearson value exceeds LOCAL_MAX - if exceeds, replace model with the current model and set the new LOCAL_MAX\n",
        "      if pearson[0] > LOCAL_MAX:\n",
        "          model[i] = clf_t\n",
        "          LOCAL_MAX = pearson[0]\n",
        "      pearson_arr.append(pearson[0])\n",
        "      print(f'RMSE: {rmse(predictions,y_val_de)} Pearson {pearson[0]}')\n",
        "\n",
        "  print(f'Average Pearson: {np.mean(pearson_arr)}')\n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNcQ-yI7k3G4",
        "colab_type": "text"
      },
      "source": [
        "## 6. Getting the test set predictions\n",
        "Based on the model, the highest pearson correlation was for the linear function so we will use the linear model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNxM932Pk6s8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test Set - X\n",
        "de_test_src = convert_to_vec(\"./test.ende.src\",bow_en,'en')\n",
        "de_test_mt = convert_to_vec(\"./test.ende.mt\",bow_de,'de')\n",
        "\n",
        "X_test_de = np.concatenate((np.array(de_test_src), np.array(de_test_mt)), 1)\n",
        "\n",
        "#Predict with best linear model\n",
        "clf_de = model[0]\n",
        "predictions_de = clf_de.predict(X_test_de)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQej-9wIlZRM",
        "colab_type": "text"
      },
      "source": [
        "## 7. Writing to File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WXwUJZylcRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "def writeScores(method_name,scores):\n",
        "  fn = \"predictions.txt\"\n",
        "  print(\"\")\n",
        "  with open(fn, 'w') as output_file:\n",
        "    for idx,x in enumerate(scores):\n",
        "      output_file.write(f\"{x}\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGc1XX2flj4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write the prediction score into a txt file\n",
        "writeScores(\"SVR\",predictions_de)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}