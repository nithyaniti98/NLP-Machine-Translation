{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rNyCP-1sC-ZG",
        "XDzetIkRUx8U",
        "Zaiz7jQ8YCkq",
        "VMYLpcW3ZDDR",
        "_2fC42MBC-aC",
        "0NXsYhnDPk8f",
        "oTZc9UncC-a1"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GQz-zIkC-Yu",
        "colab_type": "text"
      },
      "source": [
        "# Import and preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_O3Lx80_gyaK",
        "colab_type": "text"
      },
      "source": [
        "In this section, we will load packages and models required for this NLP task and the dataset of English and German sentences.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLNRPOTlDHu_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pip install required NLP packages\n",
        "!pip install bert\n",
        "!pip install -U spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jzvJwiFDjay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download spacy language models\n",
        "!spacy download en_core_web_md\n",
        "!spacy link en_core_web_md en300\n",
        "\n",
        "!spacy download de_core_news_md\n",
        "!spacy link de_core_news_md de300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkPlmKgWC-Yy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import required packages\n",
        "import numpy as np\n",
        "import torch\n",
        "import scipy\n",
        "import spacy\n",
        "import bert\n",
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import re\n",
        "import itertools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj1bfnzgC-Y4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download stopwords for each language\n",
        "nlp_de =spacy.load('de300')\n",
        "nlp_en =spacy.load('en300')\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words_en = set(stopwords.words('english'))\n",
        "stop_words_de = set(stopwords.words('german'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fhBVNxJFQ3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get training, validation and test datasets\n",
        "from os.path import exists\n",
        "if not exists('ende_data.zip'):\n",
        "    !wget -O ende_data.zip https://competitions.codalab.org/my/datasets/download/c748d2c0-d6be-4e36-9f12-ca0e88819c4d\n",
        "    !unzip ende_data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6N8pdZ5hZmb",
        "colab_type": "text"
      },
      "source": [
        "When reading the sentences from the dataset, we perform basic preprocessing which include lowercasing the words, splitting connected words and remove punctuation or symbols in the remove_list.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coVYeS6TC-Y9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "remove_list = ['\\n', '.', ',', '/', '\\'', '-', '_']\n",
        "\n",
        "# Wrapping method that extracts and preprocess sentences in a file.\n",
        "# Parameters: file. Returns: list of sentences.\n",
        "def get_sentences(f):\n",
        "    file = open(f)\n",
        "    lines = file.readlines()\n",
        "    sentences = []\n",
        "    \n",
        "    for l in lines:\n",
        "        sentences.append(preprocess(l))\n",
        "        \n",
        "    return sentences\n",
        "\n",
        "# Method to preprocess individual sentences.\n",
        "# Parameters: a single string representing a sentence. \n",
        "# Returns: a single string after pre-processing\n",
        "def preprocess(line):\n",
        "    text = line.lower()\n",
        "    l = text.split(\" \")\n",
        "    \n",
        "    for i, word in enumerate(l):\n",
        "        if '_' in word:\n",
        "            del l[i]\n",
        "            l[i:i] = re.sub('_', ' ', word).split()\n",
        "        elif '/' in word:\n",
        "            del l[i]\n",
        "            l[i:i] = re.sub('/', ' / ', word).split()\n",
        "        elif len(' '.join(word.split('-')).split())>=2:\n",
        "            del l[i]\n",
        "            l[i:i] = re.sub('-', ' ', word).split()\n",
        "    \n",
        "    l = ' '.join([word for word in l if word not in remove_list])\n",
        "    return l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnfwVXEGC-ZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en_train_sentences = get_sentences(\"./train.ende.src\")\n",
        "de_train_sentences = get_sentences(\"./train.ende.mt\")\n",
        "en_val_sentences = get_sentences(\"./dev.ende.src\")\n",
        "de_val_sentences = get_sentences(\"./dev.ende.mt\")\n",
        "\n",
        "with open(\"./train.ende.scores\", \"r\") as f:\n",
        "  train_scores = [l.rstrip('\\n') for l in f.readlines()]\n",
        "with open(\"./dev.ende.scores\", \"r\") as f:\n",
        "  val_scores = [l.rstrip('\\n') for l in f.readlines()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyirC_mclPvB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en_test_sentences = get_sentences(\"./test.ende.src\")\n",
        "de_test_sentences = get_sentences(\"./test.ende.mt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X40X22RVktlK",
        "colab_type": "text"
      },
      "source": [
        "# Word Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riyx6Gf3kxv1",
        "colab_type": "text"
      },
      "source": [
        "This section contains the different types of word embedding models used, which are separate language BERT models or a multilingual BERT model.\n",
        "To choose a specific type of word-embedding, proceed to the respective session and run the cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq0IYaOZ2-Q5",
        "colab_type": "text"
      },
      "source": [
        "## English and German BERT tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjq9i9nupebi",
        "colab_type": "text"
      },
      "source": [
        "With this tokenizer, English and German sentences are converted to word embeddings using separate models. Words within sentences are first converted to their word vectors and are then processed to ensure that the model is able to accurately identify the context of the words.\n",
        "Additionally, with this word embedding method, the Named-Entity-Recognition (NER) tag can be used as part of the pre-processing method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evtnjHt03KIV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "symbol_list = ['\\n', '$', '+', '-PRON-', '<', '=', '>', '°', '¥', '£', '\\'s']\n",
        "\n",
        "# Method that converts words in a sentence to word-embeddings. The sentence is first\n",
        "# converted to tokens and then stopwords, punctuations, symbols are removed.\n",
        "# Parameters: List of sentences, spacy language model, string value of language (either 'EN' or 'DE')\n",
        "# and a boolean to identify if entity should be replaced (if True) or removed (if False)\n",
        "# Returns: list of list of word-embeddings in each sentence\n",
        "def spacy_word_emb(corpus, nlp, lang, replace_ent=True):\n",
        "  corpus_doc = []\n",
        "  if replace_ent:\n",
        "    vec_length = 25\n",
        "  else:\n",
        "    vec_length = 23\n",
        "\n",
        "  for r, sentence in enumerate(corpus):\n",
        "    doc = nlp(sentence)\n",
        "    t = []\n",
        "    for c, token in enumerate(doc):\n",
        "      if token.is_punct or token.lemma_ in symbol_list:\n",
        "        continue\n",
        "      elif re.search('\\d', token.lemma_) is not None:\n",
        "        continue\n",
        "\n",
        "      if token.ent_type_ is \"\":\n",
        "        if lang == 'en':\n",
        "          if token.lemma_ in stop_words_en:\n",
        "            continue\n",
        "          else:\n",
        "            vocabulary_en.append(token.lemma_)\n",
        "        elif lang == 'de':\n",
        "          if token.lemma_ in stop_words_de:\n",
        "            continue\n",
        "          else:\n",
        "            vocabulary_de.append(token.lemma_)\n",
        "        t.append(token.vector_norm)\n",
        "      elif replace_ent:\n",
        "        if token.ent_type_ not in ent_to_vec:\n",
        "          random_vec = np.random.uniform(-1, 1, (300,))\n",
        "          ent_to_vec[token.ent_type_] = np.linalg.norm(random_vec)\n",
        "        t.append(ent_to_vec[token.ent_type_])\n",
        "    \n",
        "    if len(t) < vec_length:\n",
        "        t.extend([0.0 for i in range(vec_length-len(t))])\n",
        "    \n",
        "    corpus_doc.append(t)\n",
        "  return corpus_doc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUpjWupy8ce7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ent_to_vec = {} # dictionary mapping entity tag type to it's vector norm\n",
        "vocabulary_en = [] # All words in english corpus\n",
        "vocabulary_de = [] # All words in german corpus\n",
        "\n",
        "X_train_en = spacy_word_emb(en_train_sentences, nlp_en, 'en')\n",
        "X_train_de = spacy_word_emb(de_train_sentences, nlp_de, 'de')\n",
        "X_val_en = spacy_word_emb(en_val_sentences, nlp_en, 'en')\n",
        "X_val_de = spacy_word_emb(en_val_sentences, nlp_de, 'de')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0Ktw3D6leZr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test_en = spacy_word_emb(en_test_sentences, nlp_en, 'en')\n",
        "X_test_de = spacy_word_emb(de_test_sentences, nlp_de, 'de')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApHm4mYWCB1Y",
        "colab_type": "text"
      },
      "source": [
        "### Evaluating tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8ZWibnDoOfK",
        "colab_type": "text"
      },
      "source": [
        "This **optional** section checks how well words are tokenized for each language."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIW8p5TUC5hR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "pd.set_option('display.max_colwidth', -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_syae_yE-xe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocabulary_total = vocabulary_en.copy()\n",
        "vocabulary_total.extend(vocabulary_de)\n",
        "\n",
        "vocabulary = len(set(vocabulary_total)) + len(ent_to_vec) # total words in vocabulary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmG3R8yOCE3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unique_en_words, unique_en_counts = np.unique(vocabulary_en, return_counts=True)\n",
        "df_unique_en = pd.DataFrame({'word':unique_en_words, 'count':unique_en_counts}, columns=['word', 'count'])\n",
        "unique_de_words, unique_de_counts = np.unique(vocabulary_de, return_counts=True)\n",
        "df_unique_de = pd.DataFrame({'word':unique_de_words, 'count':unique_de_counts}, columns=['word', 'count'])\n",
        "\n",
        "with pd.ExcelWriter('Unique_counts.xlsx') as writer:  \n",
        "    df_unique_en.to_excel(writer, sheet_name='EN') # represents words and counts of unique english words\n",
        "    df_unique_de.to_excel(writer, sheet_name='DE') # represents words and counts of unique german words\n",
        "\n",
        "files.download('Unique_counts.xlsx')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNyCP-1sC-ZG",
        "colab_type": "text"
      },
      "source": [
        "## Multilingual bert tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEfI44v0qGpC",
        "colab_type": "text"
      },
      "source": [
        "With this tokenizer, English and German sentences are converted to word embeddings using the same model. Words within sentences are first converted to their word vectors and are then processed to ensure that the model is able to accurately identify the context of the words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcJ9Ubt2ovj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "s_tokenizer = SentenceTransformer('distiluse-base-multilingual-cased')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEe_7sZ9qXhp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_en = s_tokenizer.encode(en_train_sentences)\n",
        "X_train_de = s_tokenizer.encode(de_train_sentences)\n",
        "X_val_en = s_tokenizer.encode(en_val_sentences)\n",
        "X_val_de = s_tokenizer.encode(de_val_sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FohZBpPbqg0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test_en = s_tokenizer.encode(en_test_sentences)\n",
        "X_test_de = s_tokenizer.encode(de_test_sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDzetIkRUx8U",
        "colab_type": "text"
      },
      "source": [
        "## Part-of-Speech tag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59svSAY3qsEk",
        "colab_type": "text"
      },
      "source": [
        "In this section, the Part-of-Speech (POS) tags are extracted from each sentence and saved in lists. The tags can then be embedded to be input into the regression model as a vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXHhD9m0o9N7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Identify POS tags with respect to each sentence in the file.\n",
        "# Parameters: List of sentences, language of sentences.\n",
        "# Return: A list of list of POS tags within a sentence.\n",
        "def get_pos_tag(corpus, nlp):\n",
        "    pos_list = []\n",
        "    for sentence in corpus:\n",
        "        doc = nlp(sentence)\n",
        "        pos_list.append([token.pos_ for token in doc])\n",
        "    return pos_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNVFuvxkC-Zo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en_train_pos = get_pos_tag(en_train_sentences, nlp_en)\n",
        "de_train_pos = get_pos_tag(de_train_sentences, nlp_de)\n",
        "en_val_pos = get_pos_tag(en_val_sentences, nlp_en)\n",
        "de_val_pos = get_pos_tag(de_val_sentences, nlp_de)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTh74p__bRUh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en_test_pos = get_pos_tag(en_test_sentences, nlp_en)\n",
        "de_test_pos = get_pos_tag(de_test_sentences, nlp_de)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBxjLnufC-Z5",
        "colab_type": "text"
      },
      "source": [
        "# Prepare training samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMt9E_mqsXd5",
        "colab_type": "text"
      },
      "source": [
        "This section does the final processing of the separate English and German training vectors, scaling (if required) and PCA (if required)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI2VzfFvVUN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaled = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DetHZ-udC-Z-",
        "colab_type": "text"
      },
      "source": [
        "## Word Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4idLUxGPs0mP",
        "colab_type": "text"
      },
      "source": [
        "Currently, the word vectors for the English and German training corpuses are in separate arrays. Either the difference in the vector embeddings or the concatenated forms of the two vectors can be taken, by running the respective sections. Additionally, the validation set is merged with the training set due to the presence of KFold cross-validation during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zaiz7jQ8YCkq",
        "colab_type": "text"
      },
      "source": [
        "### Difference in vector embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adJXkQlJC-Z_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_diff = np.subtract(np.array(X_train_en), np.array(X_train_de))\n",
        "X_val_diff = np.subtract(np.array(X_val_en), np.array(X_val_de))\n",
        "\n",
        "X = np.concatenate((X_train_diff, X_val_diff), axis=0)\n",
        "y = np.concatenate((np.array(train_scores), np.array(val_scores)), axis=0).astype('float')\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTpKHDitVhjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "XTest = np.subtract(np.array(X_test_en), np.array(X_test_de))\n",
        "print(XTest.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-Bf-cc8YGWr",
        "colab_type": "text"
      },
      "source": [
        "### Concatenate vector embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c9SLrBJYJwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_en = np.concatenate((np.array(X_train_en), np.array(X_val_en)), axis=0)\n",
        "X_de = np.concatenate((np.array(X_train_de), np.array(X_val_de)), axis=0)\n",
        "\n",
        "X = np.concatenate((X_en, X_de), axis=1)\n",
        "print(X.shape)\n",
        "y = np.concatenate((np.array(train_scores), np.array(val_scores)), axis=0).astype('float')\n",
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PEavgGgl0QH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "XTest = np.concatenate((X_test_en, X_test_de), axis=1)\n",
        "print(XTest.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMYLpcW3ZDDR",
        "colab_type": "text"
      },
      "source": [
        "## Standardise data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6oVba4at360",
        "colab_type": "text"
      },
      "source": [
        "The data can **optionally** be scaled to improve the performance of the regression model or if PCA is performed at a later stage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G49lacRNZI2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaled = True\n",
        "\n",
        "scaler = {} # Save all scalers in a dictionary\n",
        "\n",
        "if X.ndim == 2:\n",
        "  scaler[0] = StandardScaler()\n",
        "  X = scaler[0].fit_transform(X)\n",
        "else:\n",
        "  for i in range(X.shape[1]): # Process each layer differently is X is 3 dimensional\n",
        "    scaler[i] = StandardScaler()\n",
        "    X[:, i, :] = scaler[i].fit_transform(X[:, i, :]) \n",
        "\n",
        "scaler['y'] = StandardScaler()\n",
        "y = np.squeeze(scaler['y'].fit_transform(y.reshape(-1,1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0Ug6ufEnVJc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if X.ndim == 2:\n",
        "  XTest = scaler[0].transform(XTest)\n",
        "else:\n",
        "  for i in range(X.shape[1]):\n",
        "    XTest[:, i, :] = scaler[i].transform(XTest[:, i, :]) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2fC42MBC-aC",
        "colab_type": "text"
      },
      "source": [
        "## Concatenate POS tag vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL2C4NRIv1uq",
        "colab_type": "text"
      },
      "source": [
        "Similar to the one-hot-encoded vector, the vector length is set to be the length of unique POS tags. However, for each sentence, the counts of the respective POS tags are added into the vector instead of the value 1 to indicate if it is present or not. \n",
        "\n",
        "Run this section if you intend for the **POS tags to be input** in the regression model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbDowrXqiSZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pos_to_ohe(l1, l2):\n",
        "    unique_tags_l1 = np.unique(list(itertools.chain.from_iterable(l1)))\n",
        "    unique_tags_l2 = np.unique(list(itertools.chain.from_iterable(l2)))\n",
        "    global_unique_tags = np.unique(np.concatenate((unique_tags_l1, unique_tags_l2), axis=0))\n",
        "    \n",
        "    ohe_tokens_l1 = np.empty((len(l1), len(global_unique_tags)))\n",
        "    for r, tok in enumerate(l1):\n",
        "        for c, tag in enumerate(global_unique_tags):\n",
        "            ohe_tokens_l1[r, c] = tok.count(tag)\n",
        "    ohe_tokens_l2 = np.empty((len(l2), len(global_unique_tags)))\n",
        "    for r, tok in enumerate(l2):\n",
        "        for c, tag in enumerate(global_unique_tags):\n",
        "            ohe_tokens_l2[r, c] = tok.count(tag)\n",
        "    \n",
        "    return ohe_tokens_l1, ohe_tokens_l2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFNUDpGYC-aD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en_tags = list(itertools.chain(en_train_pos, en_val_pos))\n",
        "de_tags = list(itertools.chain(de_train_pos, de_val_pos))\n",
        "en_tags, de_tags = pos_to_ohe(en_tags, de_tags)\n",
        "\n",
        "X_tags = np.concatenate((en_tags, de_tags), axis=1)\n",
        "X = np.concatenate((X, X_tags), axis=1)\n",
        "print(X.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZM0KwXPkbZmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en_test_tags, de_test_tags = pos_to_ohe(en_test_pos, de_test_pos)\n",
        "X_test_tags = np.concatenate((en_test_tags, de_test_tags), axis=1)\n",
        "XTest = np.concatenate((XTest, X_test_tags), axis=1)\n",
        "print(XTest.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NXsYhnDPk8f",
        "colab_type": "text"
      },
      "source": [
        "## Principal Component Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ38nQxquqDY",
        "colab_type": "text"
      },
      "source": [
        "Principal Component Analysis (PCA) can **optionally** be performed when facing a dataset with a large number of features.\n",
        "\n",
        "*Note: Data needs to be scaled before performing PCA*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vHh8vHTPnk9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NLo9h2FRM91",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(0.95) # Keep 95% of variance of dataset\n",
        "pca.fit(X[:len(en_train_sentences)])\n",
        "print(\"Number of remaining components {}\".format(pca.n_components_))\n",
        "X = pca.transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4ARLvNinnux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "XTest = pca.transform(XTest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aabaGLaDC-ag",
        "colab_type": "text"
      },
      "source": [
        "# Training regression model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaWvlqtH3mpb",
        "colab_type": "text"
      },
      "source": [
        "This section contains all the regression models used for predicting the translation score. All Keras models present will first have their parameters optimised by the package talos or a simple grid-search if the model is an sklearn model.\n",
        "\n",
        "\n",
        "Run the subsection with the model of your choice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhucHh5u4ZfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install talos"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Eg4WtHDC-ah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import talos\n",
        "from scipy.stats.stats import pearsonr\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8xZ1QyuC-ak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rmse(predictions, targets):\n",
        "    return np.sqrt(((predictions - targets) ** 2).mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myT_lQwfC-ao",
        "colab_type": "text"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12WTgbgrC-ap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "testPredictions = np.empty((8, XTest.shape[0])) #Empty array for outputing predictions on test\n",
        "i = 0\n",
        "\n",
        "for k in ['linear', 'sigmoid', 'poly', 'rbf']:\n",
        "    kf = KFold(n_splits=8, shuffle=True, random_state=42) # Having 8 splits in cross validation model\n",
        "    pearson_arr = []\n",
        "    print(k)\n",
        "    for train_index, val_index in kf.split(X):\n",
        "        Xtrain, Xval = X[train_index], X[val_index]\n",
        "        Ytrain, Yval = y[train_index], y[val_index]\n",
        "        \n",
        "        clf_t = SVR(kernel=k)\n",
        "        clf_t.fit(Xtrain, Ytrain)\n",
        "        predictions = clf_t.predict(Xval)\n",
        "        testPrediction = clf_t.predict(XTest)\n",
        "        \n",
        "        if scaled: # Transform data to original scale if data was scaled\n",
        "          predictions = scaler['y'].inverse_transform(predictions)\n",
        "          Yval = scaler['y'].inverse_transform(Yval)\n",
        "          testPrediction = scaler['y'].inverse_transform(testPrediction)\n",
        "        \n",
        "        testPredictions[i] = testPrediction\n",
        "        i += 1\n",
        "        \n",
        "        pearson = pearsonr(Yval, predictions)\n",
        "        pearson_arr.append(pearson[0])\n",
        "        print(f'RMSE: {rmse(predictions,Yval)} Pearson {pearson[0]}')\n",
        "\n",
        "    print(f'Average Pearson: {np.mean(pearson_arr)}')\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTZc9UncC-a1",
        "colab_type": "text"
      },
      "source": [
        "## RFR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8erGk81C-a2",
        "colab_type": "code",
        "outputId": "15f5b479-d575-40b3-c115-b21fa3ad6bd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "testResults = np.empty((8, XTest.shape[0]))\n",
        "i=0\n",
        "\n",
        "kf = KFold(n_splits=8, shuffle=True, random_state=42)\n",
        "pearson_arr = []\n",
        "for train_index, val_index in kf.split(X):\n",
        "    Xtrain, Xval = X[train_index], X[val_index]\n",
        "    Ytrain, Yval = y[train_index], y[val_index]\n",
        "    \n",
        "    rf = RandomForestRegressor(n_estimators = 1000, random_state = 666)\n",
        "    rf.fit(Xtrain, Ytrain)\n",
        "    predictions = rf.predict(Xval)\n",
        "    testResults[i] = rf.predict(XTest)\n",
        "    i += 1\n",
        "\n",
        "    pearson = pearsonr(Yval, predictions)\n",
        "    pearson_arr.append(pearson[0])\n",
        "    print(f'RMSE: {rmse(predictions,Yval)} Pearson {pearson[0]}')\n",
        "\n",
        "print()\n",
        "print(f'Average Pearson: {np.mean(pearson_arr)}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 0.8761084233518699 Pearson 0.09034672501500221\n",
            "RMSE: 0.7914148332181788 Pearson 0.04846836762064428\n",
            "RMSE: 0.8190332842803869 Pearson -0.00610533394767424\n",
            "RMSE: 0.8347071630107122 Pearson 0.020459422385660862\n",
            "RMSE: 0.908480919291526 Pearson 0.08223520718842349\n",
            "RMSE: 0.7979303088893454 Pearson -5.409202845798458e-05\n",
            "RMSE: 0.8631800254681851 Pearson -0.027683724937758416\n",
            "RMSE: 0.8991916843025259 Pearson 0.0005034948950817501\n",
            "\n",
            "Average Pearson: 0.026021258273865245\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZo0nKYgb39m",
        "colab_type": "text"
      },
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlniiVJ3sPgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xgboost as xgb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P01zu_RBDhNJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Method that builds and trains model\n",
        "def xgb_model(x_train, y_train, x_val, y_val=None, params):\n",
        "    model = xgb.XGBRegressor(colsample_bytree=params['colsample_bytree'],\n",
        "                 gamma=0.3,                 \n",
        "                 learning_rate=params['learning_rate'],\n",
        "                 max_depth=3,\n",
        "                 min_child_weight=params['min_child_weight'],\n",
        "                 n_estimators=10000,                                                                    \n",
        "                 reg_alpha=params['reg_alpha'],\n",
        "                 reg_lambda=params['reg_lambda'],\n",
        "                 subsample=0.6)\n",
        "    \n",
        "    if y_val is not None: # Return to talos function\n",
        "      history = model.fit(x_train, y_train, validation_data=(x_val, y_val))\n",
        "      return history, model\n",
        "    else: # Return to predict output\n",
        "      model.fit(x_train, y_train)\n",
        "      return model.predict(x_val), model\n",
        "\n",
        "xgb_params = {\n",
        "    'colsample_bytree':[0.4,0.6],\n",
        "    'min_child_weight':[1.5,6],\n",
        "    'learning_rate':[0.1,0.07],\n",
        "    'reg_alpha':[1e-5, 1e-2],\n",
        "    'reg_lambda':[1e-5, 0.45],\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jM1VTbFKGy6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = talos.Scan(x=X, y=y, model=xgb_model, fraction_limit=0.05,\n",
        "               params=xgb_params, experiment_name='XGB')\n",
        "r = talos.Reporting(t)\n",
        "xgb_best_params = r.best_params\n",
        "# r.data.sort_values(by=['val_mean_squared_error']) #run if you want to receive table of tests"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izykNB3IcBqb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kf = KFold(n_splits=8, shuffle=True, random_state=42)\n",
        "pearson_arr = []\n",
        "testResults = np.empty((8, XTest.shape[0]))\n",
        "i=0\n",
        "\n",
        "for train_index, val_index in kf.split(X):\n",
        "    Xtrain, Xval = X[train_index], X[val_index]\n",
        "    Ytrain, Yval = y[train_index], y[val_index]\n",
        "    \n",
        "    predictions, xgb_model = xgb_model(Xtrain, Ytrain, Xval, None, xgb_best_params)\n",
        "    testResult = xgb_model.predict(XTest)\n",
        "\n",
        "    if scaled:\n",
        "      predictions = scaler['y'].inverse_transform(predictions)\n",
        "      Yval = scaler['y'].inverse_transform(Yval)\n",
        "      testResult = scaler['y'].inverse_transform(testResult)\n",
        "    \n",
        "    testResults[i] = testResult\n",
        "    i += 1\n",
        "\n",
        "    pearson = pearsonr(Yval, predictions)\n",
        "    pearson_arr.append(pearson[0])\n",
        "    print(f'RMSE: {rmse(predictions,Yval)} Pearson {pearson[0]}')\n",
        "\n",
        "print()\n",
        "print(f'Average Pearson: {np.mean(pearson_arr)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmeyemZ3kPRt",
        "colab_type": "text"
      },
      "source": [
        "# Evaluating Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYFB0BiEJMcm",
        "colab_type": "text"
      },
      "source": [
        "This **optional** section evaluates the predictions by the model and identifies the best and worst performing sentences in terms of guaging the accuracy of translation and range of scores predicted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVBgjWvNkU7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en_corpus = en_train_sentences.copy()\n",
        "en_corpus.extend(en_val_sentences)\n",
        "\n",
        "de_corpus = de_train_sentences.copy()\n",
        "de_corpus.extend(de_val_sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeqEUA3blvP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Methhod that finds the most well-predicted and worst-predicted scores of translation\n",
        "# Parameters: list of model predicted scores, list of actual scores, indexes within the training corpus,\n",
        "# the number of best/worst predicted sentences to output, to get the worst (True) or best (False) scores\n",
        "# Returns a dataframe of the best/worst English and German sentences with the actual and predicted score\n",
        "def extremes(predictons, target, val_ind, n=5, mistakes=True):\n",
        "    diff = np.array(predictions)-np.array(target)\n",
        "    a, b, c, d = (list(t) for t in zip(*sorted(zip(np.abs(diff), val_ind, predictions, target))))\n",
        "    df = pd.DataFrame(columns=['EN', 'DE', 'Actual', 'Pred'])\n",
        "    \n",
        "    for i in range(1, n+1):\n",
        "        if mistakes:\n",
        "          i = -i\n",
        "        pred = c[i]\n",
        "        actual = d[i]\n",
        "        ind = b[i]\n",
        "        en_sentence = en_corpus[ind]\n",
        "        de_sentence = de_corpus[ind]\n",
        "        df = df.append(pd.Series([en_sentence, de_sentence, actual, pred], index=df.columns), \n",
        "                       ignore_index=True)\n",
        "        \n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9W0ayJFmF5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.hist(predictions, bins=20) # To plot range of values output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70ZsOsq3l62m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "extremes(predictions, Yval, val_index, 50, True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhQRVrfWollX",
        "colab_type": "text"
      },
      "source": [
        "# Output Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i78WAeEsK1Gk",
        "colab_type": "text"
      },
      "source": [
        "This section takes the mean out of the 8 predictions during the KFold cross validation and writes it into a text file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdO8Wyr7owqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testResult = np.mean(testResults, axis=0)\n",
        "plt.hist(testResult, bins=20)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqODOKpqoodS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "def writeScores(scores):\n",
        "    print(\"\")\n",
        "    with open(fn, 'w') as output_file:\n",
        "        for idx,x in enumerate(scores):\n",
        "            output_file.write(f\"{x}\\n\")\n",
        "\n",
        "fn = \"predictions.txt\"\n",
        "writeScores(testResult)\n",
        "files.download(fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSIJ685DovsL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with ZipFile(\"en-de_svr.zip\",\"w\") as newzip:\n",
        "\tnewzip.write(\"predictions.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}